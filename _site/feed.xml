<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-01-19T14:00:50-05:00</updated><id>http://localhost:4000/</id><title type="html">Adventures in Machine Learning and Biology</title><author><name>Kevin Kaichuang Yang (楊凱筌)</name></author><entry><title type="html">A Journey from Keras to Pytorch</title><link href="http://localhost:4000/2018/03/26/pytorch-from-keras.html" rel="alternate" type="text/html" title="A Journey from Keras to Pytorch" /><published>2018-03-26T04:00:00-04:00</published><updated>2018-03-26T04:00:00-04:00</updated><id>http://localhost:4000/2018/03/26/pytorch-from-keras</id><content type="html" xml:base="http://localhost:4000/2018/03/26/pytorch-from-keras.html">&lt;p&gt;Let’s say, hypothetically speaking, that some inane taskmaster gives you a trained model in &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt; and asks you to convert it to &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt;, keeping the learned weights.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;model-dimension-hell&quot;&gt;Model dimension hell&lt;/h2&gt;

&lt;h2 id=&quot;diving-into-the-batchnorm-documentation&quot;&gt;Diving into the Batchnorm documentation&lt;/h2&gt;

&lt;h2 id=&quot;linear-algebra-balrog&quot;&gt;Linear algebra Balrog&lt;/h2&gt;

&lt;h2 id=&quot;dimension-hell-part-ii&quot;&gt;Dimension hell, part II&lt;/h2&gt;</content><author><name>Kevin Kaichuang Yang (楊凱筌)</name></author><summary type="html">Let’s say, hypothetically speaking, that some inane taskmaster gives you a trained model in Keras and asks you to convert it to PyTorch, keeping the learned weights.</summary></entry><entry><title type="html">Learning the language of proteins</title><link href="http://localhost:4000/2018/03/26/learning-the-language-of-proteins.html" rel="alternate" type="text/html" title="Learning the language of proteins" /><published>2018-03-26T04:00:00-04:00</published><updated>2018-03-26T04:00:00-04:00</updated><id>http://localhost:4000/2018/03/26/learning-the-language-of-proteins</id><content type="html" xml:base="http://localhost:4000/2018/03/26/learning-the-language-of-proteins.html">&lt;p&gt;Amino acids in a protein are analogous to letters in an alphabet, short subsequences of amino acids are analogous to words in an unknown language, and a protein’s entire amino-acid sequence to a document encoding its structure and function. Therefore, I applied techniques from natural language processing to learn the language of proteins. Given a large collection of unlabeled texts, &lt;a href=&quot;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot;&gt;word2vec&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1405.4053&quot;&gt;doc2vec&lt;/a&gt; are well-established methods that learn to convert words and documents to vectors that capture their meaning. Similarly, using a large collection of unlabeled protein sequences, I trained a model that learns to vectorize proteins such that similar vectors encode similar proteins. These vectors can then be used in machine-learning models that predict protein properties from small amounts of labeled data. Models built using these vectors predict protein properties without explicit knowledge of the protein’s structure or the properties of its amino acids. Instead, the encoding process transfers information from the unlabeled sequences to the prediction problem.&lt;/p&gt;

&lt;h2 id=&quot;learn-to-encode-proteins&quot;&gt;Learn to encode proteins&lt;/h2&gt;

&lt;p&gt;I spent the first part of my PhD using machine learning to &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005786&quot;&gt;predict protein properties&lt;/a&gt; from small sets of measured sequences. The first step of a machine-learning pipeline for proteins is &lt;em&gt;encoding&lt;/em&gt; the proteins. We used what’s known as a one-hot encoding. For example, if I want to encode the DNA sequence AGTT, then I can encode each position using three 0s and one 1. Of course, for proteins, I would need 19 zeros and one 1 for each position.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/onehot.jpg&quot; alt=&quot;One-hot encoding&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One-hot encodings are a good default. They do, however, have some drawbacks. One-hot encodings are space-inefficient and don’t account for the amino acid properties. There are lists and tables of amino acid properties, but different problems will require different properties, and it’s not obvious beforehand what those are. One-hot encodings are also surprisingly difficult to implement correctly. This quarter, I TAd a class where we had the students reproduce many of my results. Helping them with their homework, I noticed that the single most difficult part was making the one-hot encodings of the sequences. This gets even worse when you don’t have a TA who picks out all the sequences for you and pre-aligns them!  So if we’re training models to predict protein properties from data, why not train models to encode the proteins too?&lt;/p&gt;

&lt;p&gt;Of course, I’m not the first person to have this idea. In natural language processing, there’s an analogous problem when encoding sequences of words. Starting with word2vec, people have used learned vectors to represent words and sentences. (These are often known as ‘embeddings’ because they ‘embed’ words into a vector space). Other people have explained these in much more detail (and much better than I can). For example: &lt;a href=&quot;https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/&quot;&gt;word2vec&lt;/a&gt;, &lt;a href=&quot;https://blog.acolyer.org/2016/06/01/distributed-representations-of-sentences-and-documents/&quot;&gt;doc2vec&lt;/a&gt;. These all use the assumption that similar words occur in similar contexts to learn embeddings that place words with similar meanings close together in the embedded space. They’re generally trained on massive datasets, such as all of Wikipedia. This means that, in addition to providing a convenient representation, they also encode information from massive (and free) unlabeled datasets, which improves performance on a variety of tasks.&lt;/p&gt;

&lt;p&gt;For example, if I want to train a model to learn whether movie reviews are positive or negative, I need a dataset of reviews that are labeled as positive or negative. That usually means that somebody has to manually label the reviews, which puts an upper limit on how many examples I can obtain. On the other hand, downloading all of Wikipedia is quick and easy. Likewise, I work with small sets (&amp;lt; 300 sequences-label pairs) of labeled protein sequences, but I can also go and download 500,000+ protein sequences from UniProt.&lt;/p&gt;

&lt;p&gt;Inspired by doc2vec, I created a two-part pipeline for embedding sequences of interest. First, unsupervised doc2vec embedding models were trained on proteins downloaded from UniProt. Instead of ‘words,’ we have ‘k-mers’, which I obtain by chopping each protein up into subsequences of length &lt;em&gt;k&lt;/em&gt;. For example, for k = 3, there are three lists and each list begins at one of the first three amino acid positions of the sequence. Doc2vec learns an embedding for each overall sequence and for each k-mer. I chose to use 64-dimensional embeddings.&lt;/p&gt;

&lt;p&gt;Once I have this unsupervised embedding model, I can use it to encode sequences of interest by first chopping them into k-mers of the same size. These encodings then serve as inputs in &lt;a href=&quot;http://www.gaussianprocess.org/gpml/chapters/RW.pdf&quot;&gt;Gaussian process (pdf)&lt;/a&gt; regression models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/figure1.png&quot; alt=&quot;Scheme&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;encodings-enable-accurate-models&quot;&gt;Encodings enable accurate models&lt;/h2&gt;

&lt;p&gt;I tested embeddings on four protein prediction tasks, comparing their performance to one-hot encodings of sequence or sequence and structure, encodings based on physical properties (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/9847231&quot;&gt;AAIndex&lt;/a&gt; and &lt;a href=&quot;https://academic.oup.com/bioinformatics/article/31/21/3429/194375&quot;&gt;ProFET&lt;/a&gt;), and &lt;a href=&quot;https://academic.oup.com/bioinformatics/article/20/4/467/192308&quot;&gt;string mismatch kernels&lt;/a&gt;. For all of these tasks, the embeddings performed at least as well as the other methods despite not using alignments, structural information, or physical properties. For example, here are the test predictions for channelrhodopsin localization, where embeddings have both the highest Kendall Tau and lowest mean absolute deviation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/localization_predictions.jpg&quot; alt=&quot;Predictions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I chose to use 64-dimensional embeddings, but it turns out that I can still train reasonably accurate models after reducing the number of embedding dimensions to 16.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/vary_d_curve.jpg&quot; alt=&quot;d curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Think about this for a minute. The protein of interest has over 200 amino acids. Protein translation and trafficking is a multi-step, poorly-understood process. &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005786&quot;&gt;Previously&lt;/a&gt;, we had found that there are no simple sequence or structure predictors of membrane localization. But I can store enough information in 16 numbers to predict how well it will be trafficked to the cell plasma membrane.&lt;/p&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further reading&lt;/h2&gt;

&lt;p&gt;The full paper is available at Bioinformatics:  &lt;a href=&quot;https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty178/4951834?guestAccessKey=aa420938-7c4a-4c47-8763-bad82d936d10&quot;&gt;Learned protein embeddings for machine learning&lt;/a&gt;, and code to reproduce the results is available &lt;a href=&quot;https://github.com/fhalab/embeddings_reproduction&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kevin Kaichuang Yang (楊凱筌)</name></author><summary type="html">Amino acids in a protein are analogous to letters in an alphabet, short subsequences of amino acids are analogous to words in an unknown language, and a protein’s entire amino-acid sequence to a document encoding its structure and function. Therefore, I applied techniques from natural language processing to learn the language of proteins. Given a large collection of unlabeled texts, word2vec and doc2vec are well-established methods that learn to convert words and documents to vectors that capture their meaning. Similarly, using a large collection of unlabeled protein sequences, I trained a model that learns to vectorize proteins such that similar vectors encode similar proteins. These vectors can then be used in machine-learning models that predict protein properties from small amounts of labeled data. Models built using these vectors predict protein properties without explicit knowledge of the protein’s structure or the properties of its amino acids. Instead, the encoding process transfers information from the unlabeled sequences to the prediction problem.</summary></entry></feed>